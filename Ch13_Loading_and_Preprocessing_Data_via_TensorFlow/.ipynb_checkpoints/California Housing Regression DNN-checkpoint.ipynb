{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tarfile\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Price Regression Model - Keras and TensorFlow Practice\n",
    "\n",
    "Christian Smith\n",
    "\n",
    "**Electronic Systems Lab, Georgia Tech Research Institute**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Use the California Housing Dataset available in Aurelie Geron's GitHub account to build a DNN that will act as a regressor to predict housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "### Downloading and Showing the Data\n",
    "\n",
    "Below, we define the code to download and load the dataset (altered only slightly from Geron's original code). We show it in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the California housing dataset\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  \n",
       "2       496.0       177.0         7.2574            352100.0  \n",
       "3       558.0       219.0         5.6431            341300.0  \n",
       "4       565.0       259.0         3.8462            342200.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the last column\n",
    "housing = load_housing_data()\n",
    "housing = housing.iloc[:, :-1]\n",
    "housing.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and Splitting the Data\n",
    "\n",
    "Here is the start of my contribution. I've shuffled the dataset and will split them into training, validation, and test DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the rows and splitting into training, validation, test sets\n",
    "housing = housing.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "num_rows = len(housing.index)\n",
    "train_rows = int(np.floor(num_rows*0.6))\n",
    "valid_rows = int(np.floor(num_rows*0.2))\n",
    "test_rows = int(np.floor(num_rows*0.2))\n",
    "\n",
    "housing_train = housing.iloc[0:train_rows, :]\n",
    "housing_valid = housing.iloc[train_rows:(train_rows+valid_rows), :]\n",
    "housing_test = housing.iloc[(train_rows+valid_rows):(train_rows+valid_rows+test_rows), :]\n",
    "\n",
    "housing_train.reset_index(drop=True, inplace=True)\n",
    "housing_valid.reset_index(drop=True, inplace=True)\n",
    "housing_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-118.19</td>\n",
       "      <td>34.03</td>\n",
       "      <td>31.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>2.6964</td>\n",
       "      <td>125000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-119.85</td>\n",
       "      <td>36.82</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>5.5842</td>\n",
       "      <td>88900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-118.60</td>\n",
       "      <td>34.26</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6154.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>3010.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>5.6392</td>\n",
       "      <td>271500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-118.39</td>\n",
       "      <td>34.06</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3299.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>759.0</td>\n",
       "      <td>3.3295</td>\n",
       "      <td>500001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.12</td>\n",
       "      <td>37.44</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1509.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>4.8750</td>\n",
       "      <td>373400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -118.19     34.03                31.0        525.0           136.0   \n",
       "1    -119.85     36.82                15.0       1387.0           236.0   \n",
       "2    -118.60     34.26                18.0       6154.0          1070.0   \n",
       "3    -118.39     34.06                39.0       3299.0           831.0   \n",
       "4    -122.12     37.44                33.0       1509.0           303.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       627.0       145.0         2.6964            125000.0  \n",
       "1       638.0       195.0         5.5842             88900.0  \n",
       "2      3010.0      1034.0         5.6392            271500.0  \n",
       "3      1649.0       759.0         3.3295            500001.0  \n",
       "4       748.0       268.0         4.8750            373400.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12384, 9)\n",
      "(4128, 9)\n",
      "(4128, 9)\n"
     ]
    }
   ],
   "source": [
    "print(housing_train.shape)\n",
    "print(housing_valid.shape)\n",
    "print(housing_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Geron's \"Hands on Machine Learning...\" book, he introduces manipulation of data pipelining with TensorFlow by postulating a situation with multiple .csv files and interleaving them. As such, I wrote a function to write these training, validation, and test DataFrames into separate .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory to split housing data into separate .csv files for interleaving\n",
    "SPLIT_HOUSING_DIR = os.path.join(HOUSING_PATH, \"housing_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation function for splitting housing DataFrame\n",
    "def separate_housing_dat_to_csvs(df, n_csv_files, output_dir, filetitle):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    all_fp = []\n",
    "    \n",
    "    for file in range(n_csv_files):\n",
    "        full_path = os.path.join(output_dir, f\"{filetitle}_{file}.csv\")\n",
    "        df[np.arange(len(df.index))//n_csv_files==file].to_csv(full_path, index=False)\n",
    "        all_fp.append(full_path)\n",
    "        \n",
    "    return all_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set and save fp\n",
    "train_fp = separate_housing_dat_to_csvs(df=housing_train,\n",
    "                                        n_csv_files=50,\n",
    "                                        output_dir=SPLIT_HOUSING_DIR,\n",
    "                                        filetitle=\"my_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split validation set and save fp\n",
    "valid_fp = separate_housing_dat_to_csvs(df=housing_train,\n",
    "                                        n_csv_files=25,\n",
    "                                        output_dir=SPLIT_HOUSING_DIR,\n",
    "                                        filetitle=\"my_valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test set and save fp\n",
    "test_fp = separate_housing_dat_to_csvs(df=housing_train,\n",
    "                                       n_csv_files=25,\n",
    "                                       output_dir=SPLIT_HOUSING_DIR,\n",
    "                                       filetitle=\"my_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaving and Preprocessing via TensorFlow\n",
    "\n",
    "Again, inspired directly from Geron, the following code will interleave these multiple .csv files and use them to create separate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filepath TensorFlow Dataset object\n",
    "train_fp_dataset = tf.data.Dataset.list_files(train_fp, seed=42)\n",
    "valid_fp_dataset = tf.data.Dataset.list_files(valid_fp, seed=42)\n",
    "test_fp_dataset = tf.data.Dataset.list_files(test_fp, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interleave rows from training filepaths\n",
    "train_dataset = train_fp_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interleave rows from validation filepaths\n",
    "valid_dataset = valid_fp_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interleave rows from test filepaths\n",
    "test_dataset = test_fp_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-118.28,34.08,40.0,1630.0,543.0,1568.0,510.0,2.7366,169100.0'\n",
      "b'-122.4,37.81,12.0,1349.0,349.0,536.0,334.0,7.7852,250000.0'\n",
      "b'-119.69,36.25,35.0,2011.0,349.0,970.0,300.0,2.395,94100.0'\n",
      "b'-119.74,34.35,34.0,1664.0,292.0,705.0,257.0,5.0,329400.0'\n",
      "b'-122.34,37.58,50.0,2784.0,743.0,1622.0,698.0,3.8413,372200.0'\n"
     ]
    }
   ],
   "source": [
    "for line in train_dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these have been defined, the preprocessing method can be written. The majority was written by Geron, but his code did not define how it was to be scaled (and it did not scale the output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of input features\n",
    "n_inputs = 8\n",
    "\n",
    "# Find the \n",
    "X_mean_series = housing.mean(axis=0) \n",
    "X_std_series = housing.std(axis=0)\n",
    "\n",
    "X_mean = np.asarray(X_mean_series.values)\n",
    "X_std = np.asarray(X_std_series.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude               -119.569704\n",
      "latitude                  35.631861\n",
      "housing_median_age        28.639486\n",
      "total_rooms             2635.763081\n",
      "total_bedrooms           537.870553\n",
      "population              1425.476744\n",
      "households               499.539680\n",
      "median_income              3.870671\n",
      "median_house_value    206855.816909\n",
      "dtype: float64\n",
      "[-1.19569704e+02  3.56318614e+01  2.86394864e+01  2.63576308e+03\n",
      "  5.37870553e+02  1.42547674e+03  4.99539680e+02  3.87067100e+00\n",
      "  2.06855817e+05]\n"
     ]
    }
   ],
   "source": [
    "print(X_mean_series)\n",
    "print(X_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude                  2.003532\n",
      "latitude                   2.135952\n",
      "housing_median_age        12.585558\n",
      "total_rooms             2181.615252\n",
      "total_bedrooms           421.385070\n",
      "population              1132.462122\n",
      "households               382.329753\n",
      "median_income              1.899822\n",
      "median_house_value    115395.615874\n",
      "dtype: float64\n",
      "[2.00353172e+00 2.13595240e+00 1.25855576e+01 2.18161525e+03\n",
      " 4.21385070e+02 1.13246212e+03 3.82329753e+02 1.89982172e+00\n",
      " 1.15395616e+05]\n"
     ]
    }
   ],
   "source": [
    "print(X_std_series)\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "  fields = tf.io.decode_csv(line, record_defaults=defs, select_cols=[i for i in range(n_inputs+1)])\n",
    "  x = tf.stack(fields[:-1])\n",
    "  y = tf.stack(fields[-1])\n",
    "  return (x - X_mean[:-1]) / X_std[:-1], (y - X_mean[-1]) / X_std[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=2, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various datasets are then defined by the csv_reader_dataset() method pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_fp)\n",
    "valid_set = csv_reader_dataset(valid_fp)\n",
    "test_set = csv_reader_dataset(test_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "### Building the Model\n",
    "\n",
    "I created a method to construct the model in Keras. The graph of the model can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a DNN for housing price prediction\n",
    "def build_housing_reg_model(n_hidden=3,\n",
    "                            n_neurons=50,\n",
    "                            learning_rate=0.0005,\n",
    "                            beta_1=0.9,\n",
    "                            beta_2=0.999,\n",
    "                            input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden-1):\n",
    "        model.add(keras.layers.Dense(n_neurons,\n",
    "                                     activation=\"relu\",\n",
    "                                     kernel_initializer=\"he_normal\"))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(rate=0.1))\n",
    "    model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_reg_model = build_housing_reg_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                450       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,001\n",
      "Trainable params: 5,801\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "housing_reg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(housing_reg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Dropout is active during training, so I need to change the evaluation of the training loss to do so without dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model and Visualizing with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to create log directory for the\n",
    "# runs at various datetimes\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TensorBoard callback\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 19980), started 3:34:21 ago. (Use '!kill 19980' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-10d4b0fcbaf5bef2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-10d4b0fcbaf5bef2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "      1/Unknown - 3s 3s/step - loss: 1.6313WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.158580). Check your callbacks.\n",
      "79/79 [==============================] - 4s 49ms/step - loss: 0.9775 - val_loss: 0.0000e+00\n",
      "Epoch 2/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.5891 - val_loss: 0.4775\n",
      "Epoch 3/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.5334 - val_loss: 0.4127\n",
      "Epoch 4/250\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.4853 - val_loss: 0.3849\n",
      "Epoch 5/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.4572 - val_loss: 0.3507\n",
      "Epoch 6/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.4241 - val_loss: 0.3382\n",
      "Epoch 7/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.4125 - val_loss: 0.3200\n",
      "Epoch 8/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3958 - val_loss: 0.3209\n",
      "Epoch 9/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.3829 - val_loss: 0.2985\n",
      "Epoch 10/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3769 - val_loss: 0.2905\n",
      "Epoch 11/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.3877 - val_loss: 0.2707\n",
      "Epoch 12/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.3477 - val_loss: 0.2763\n",
      "Epoch 13/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3660 - val_loss: 0.2760\n",
      "Epoch 14/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3562 - val_loss: 0.2672\n",
      "Epoch 15/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.3388 - val_loss: 0.2731\n",
      "Epoch 16/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3469 - val_loss: 0.2608\n",
      "Epoch 17/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3354 - val_loss: 0.2562\n",
      "Epoch 18/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3225 - val_loss: 0.2537\n",
      "Epoch 19/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3284 - val_loss: 0.2502\n",
      "Epoch 20/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.3241 - val_loss: 0.2525\n",
      "Epoch 21/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3203 - val_loss: 0.2456\n",
      "Epoch 22/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.3206 - val_loss: 0.2505\n",
      "Epoch 23/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.3120 - val_loss: 0.2485\n",
      "Epoch 24/250\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.3146 - val_loss: 0.2467\n",
      "Epoch 25/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3077 - val_loss: 0.2442\n",
      "Epoch 26/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3037 - val_loss: 0.2460\n",
      "Epoch 27/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3102 - val_loss: 0.2420\n",
      "Epoch 28/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3180 - val_loss: 0.2444\n",
      "Epoch 29/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.3126 - val_loss: 0.2543\n",
      "Epoch 30/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.3178 - val_loss: 0.2541\n",
      "Epoch 31/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3052 - val_loss: 0.2395\n",
      "Epoch 32/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.3242 - val_loss: 0.2391\n",
      "Epoch 33/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.3189 - val_loss: 0.2344\n",
      "Epoch 34/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2976 - val_loss: 0.2443\n",
      "Epoch 35/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2996 - val_loss: 0.2329\n",
      "Epoch 36/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2946 - val_loss: 0.2342\n",
      "Epoch 37/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3067 - val_loss: 0.2397\n",
      "Epoch 38/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2833 - val_loss: 0.2374\n",
      "Epoch 39/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2795 - val_loss: 0.2457\n",
      "Epoch 40/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2822 - val_loss: 0.2345\n",
      "Epoch 41/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2816 - val_loss: 0.2376\n",
      "Epoch 42/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2679 - val_loss: 0.2324\n",
      "Epoch 43/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2966 - val_loss: 0.2304\n",
      "Epoch 44/250\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.2744 - val_loss: 0.2334\n",
      "Epoch 45/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2718 - val_loss: 0.2453\n",
      "Epoch 46/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2916 - val_loss: 0.2303\n",
      "Epoch 47/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2928 - val_loss: 0.2326\n",
      "Epoch 48/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2687 - val_loss: 0.2363\n",
      "Epoch 49/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2720 - val_loss: 0.2403\n",
      "Epoch 50/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2764 - val_loss: 0.2287\n",
      "Epoch 51/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2779 - val_loss: 0.2321\n",
      "Epoch 52/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2731 - val_loss: 0.2312\n",
      "Epoch 53/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2873 - val_loss: 0.2261\n",
      "Epoch 54/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2670 - val_loss: 0.2249\n",
      "Epoch 55/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2817 - val_loss: 0.2243\n",
      "Epoch 56/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2764 - val_loss: 0.2397\n",
      "Epoch 57/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2732 - val_loss: 0.2249\n",
      "Epoch 58/250\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2663 - val_loss: 0.2313\n",
      "Epoch 59/250\n",
      "79/79 [==============================] - 1s 13ms/step - loss: 0.2695 - val_loss: 0.2201\n",
      "Epoch 60/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2774 - val_loss: 0.2261\n",
      "Epoch 61/250\n",
      "79/79 [==============================] - 1s 13ms/step - loss: 0.2604 - val_loss: 0.2225\n",
      "Epoch 62/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2797 - val_loss: 0.2299\n",
      "Epoch 63/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2564 - val_loss: 0.2168\n",
      "Epoch 64/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2621 - val_loss: 0.2198\n",
      "Epoch 65/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2558 - val_loss: 0.2153\n",
      "Epoch 66/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2705 - val_loss: 0.2186\n",
      "Epoch 67/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2686 - val_loss: 0.2235\n",
      "Epoch 68/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2648 - val_loss: 0.2152\n",
      "Epoch 69/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2572 - val_loss: 0.2168\n",
      "Epoch 70/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2458 - val_loss: 0.2210\n",
      "Epoch 71/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2607 - val_loss: 0.2229\n",
      "Epoch 72/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2597 - val_loss: 0.2191\n",
      "Epoch 73/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2677 - val_loss: 0.2159\n",
      "Epoch 74/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2817 - val_loss: 0.2168\n",
      "Epoch 75/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2465 - val_loss: 0.2228\n",
      "Epoch 76/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2698 - val_loss: 0.2136\n",
      "Epoch 77/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2613 - val_loss: 0.2125\n",
      "Epoch 78/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2615 - val_loss: 0.2217\n",
      "Epoch 79/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2592 - val_loss: 0.2126\n",
      "Epoch 80/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2451 - val_loss: 0.2165\n",
      "Epoch 81/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2488 - val_loss: 0.2186\n",
      "Epoch 82/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2468 - val_loss: 0.2210\n",
      "Epoch 83/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2572 - val_loss: 0.2128\n",
      "Epoch 84/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2525 - val_loss: 0.2100\n",
      "Epoch 85/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2592 - val_loss: 0.2101\n",
      "Epoch 86/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2529 - val_loss: 0.2163\n",
      "Epoch 87/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2593 - val_loss: 0.2073\n",
      "Epoch 88/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2536 - val_loss: 0.2107\n",
      "Epoch 89/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2471 - val_loss: 0.2127\n",
      "Epoch 90/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2492 - val_loss: 0.2191\n",
      "Epoch 91/250\n",
      "79/79 [==============================] - 1s 17ms/step - loss: 0.2345 - val_loss: 0.2120\n",
      "Epoch 92/250\n",
      "79/79 [==============================] - 1s 15ms/step - loss: 0.2651 - val_loss: 0.2117\n",
      "Epoch 93/250\n",
      "79/79 [==============================] - 1s 15ms/step - loss: 0.2598 - val_loss: 0.2174\n",
      "Epoch 94/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2524 - val_loss: 0.2137\n",
      "Epoch 95/250\n",
      "79/79 [==============================] - 1s 15ms/step - loss: 0.2396 - val_loss: 0.2099\n",
      "Epoch 96/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2361 - val_loss: 0.2051\n",
      "Epoch 97/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2406 - val_loss: 0.2065\n",
      "Epoch 98/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2506 - val_loss: 0.2027\n",
      "Epoch 99/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2470 - val_loss: 0.2063\n",
      "Epoch 100/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2570 - val_loss: 0.2033\n",
      "Epoch 101/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2456 - val_loss: 0.2084\n",
      "Epoch 102/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2440 - val_loss: 0.2107\n",
      "Epoch 103/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2376 - val_loss: 0.2095\n",
      "Epoch 104/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2495 - val_loss: 0.2031\n",
      "Epoch 105/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2519 - val_loss: 0.2071\n",
      "Epoch 106/250\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2411 - val_loss: 0.1995\n",
      "Epoch 107/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2358 - val_loss: 0.2070\n",
      "Epoch 108/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2515 - val_loss: 0.2146\n",
      "Epoch 109/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2463 - val_loss: 0.2052\n",
      "Epoch 110/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2305 - val_loss: 0.1978\n",
      "Epoch 111/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2369 - val_loss: 0.2089\n",
      "Epoch 112/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2435 - val_loss: 0.2064\n",
      "Epoch 113/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2427 - val_loss: 0.1963\n",
      "Epoch 114/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2338 - val_loss: 0.1989\n",
      "Epoch 115/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2425 - val_loss: 0.1967\n",
      "Epoch 116/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2408 - val_loss: 0.2024\n",
      "Epoch 117/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2388 - val_loss: 0.1952\n",
      "Epoch 118/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2343 - val_loss: 0.1938\n",
      "Epoch 119/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2404 - val_loss: 0.1987\n",
      "Epoch 120/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2356 - val_loss: 0.2040\n",
      "Epoch 121/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2349 - val_loss: 0.1955\n",
      "Epoch 122/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2372 - val_loss: 0.2024\n",
      "Epoch 123/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2220 - val_loss: 0.1988\n",
      "Epoch 124/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2395 - val_loss: 0.1976\n",
      "Epoch 125/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2288 - val_loss: 0.1958\n",
      "Epoch 126/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2470 - val_loss: 0.1971\n",
      "Epoch 127/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2413 - val_loss: 0.2006\n",
      "Epoch 128/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2376 - val_loss: 0.1912\n",
      "Epoch 129/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2246 - val_loss: 0.1960\n",
      "Epoch 130/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2342 - val_loss: 0.1900\n",
      "Epoch 131/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2424 - val_loss: 0.1938\n",
      "Epoch 132/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2303 - val_loss: 0.1885\n",
      "Epoch 133/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2444 - val_loss: 0.1914\n",
      "Epoch 134/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2402 - val_loss: 0.1938\n",
      "Epoch 135/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2354 - val_loss: 0.1890\n",
      "Epoch 136/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.1982\n",
      "Epoch 137/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2255 - val_loss: 0.1897\n",
      "Epoch 138/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2389 - val_loss: 0.1952\n",
      "Epoch 139/250\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.2269 - val_loss: 0.1896\n",
      "Epoch 140/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2359 - val_loss: 0.1863\n",
      "Epoch 141/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2438 - val_loss: 0.1839\n",
      "Epoch 142/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2273 - val_loss: 0.1887\n",
      "Epoch 143/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2342 - val_loss: 0.1870\n",
      "Epoch 144/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2386 - val_loss: 0.1895\n",
      "Epoch 145/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2265 - val_loss: 0.1888\n",
      "Epoch 146/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.1884\n",
      "Epoch 147/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2362 - val_loss: 0.1898\n",
      "Epoch 148/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2381 - val_loss: 0.1880\n",
      "Epoch 149/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2270 - val_loss: 0.1908\n",
      "Epoch 150/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2361 - val_loss: 0.1889\n",
      "Epoch 151/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2309 - val_loss: 0.1881\n",
      "Epoch 152/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2397 - val_loss: 0.1867\n",
      "Epoch 153/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2281 - val_loss: 0.1893\n",
      "Epoch 154/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2301 - val_loss: 0.1841\n",
      "Epoch 155/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2292 - val_loss: 0.1830\n",
      "Epoch 156/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2406 - val_loss: 0.1851\n",
      "Epoch 157/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2291 - val_loss: 0.1796\n",
      "Epoch 158/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2189 - val_loss: 0.1808\n",
      "Epoch 159/250\n",
      "79/79 [==============================] - 1s 6ms/step - loss: 0.2342 - val_loss: 0.1763\n",
      "Epoch 160/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2209 - val_loss: 0.1787\n",
      "Epoch 161/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2240 - val_loss: 0.1795\n",
      "Epoch 162/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2305 - val_loss: 0.1779\n",
      "Epoch 163/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2161 - val_loss: 0.1818\n",
      "Epoch 164/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2258 - val_loss: 0.1759\n",
      "Epoch 165/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2345 - val_loss: 0.1898\n",
      "Epoch 166/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2336 - val_loss: 0.1766\n",
      "Epoch 167/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2216 - val_loss: 0.1816\n",
      "Epoch 168/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2126 - val_loss: 0.1800\n",
      "Epoch 169/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2317 - val_loss: 0.1769\n",
      "Epoch 170/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2186 - val_loss: 0.1761\n",
      "Epoch 171/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2302 - val_loss: 0.1773\n",
      "Epoch 172/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2217 - val_loss: 0.1765\n",
      "Epoch 173/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2156 - val_loss: 0.1777\n",
      "Epoch 174/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2060 - val_loss: 0.1746\n",
      "Epoch 175/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2236 - val_loss: 0.1763\n",
      "Epoch 176/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2270 - val_loss: 0.1762\n",
      "Epoch 177/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2193 - val_loss: 0.1811\n",
      "Epoch 178/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2255 - val_loss: 0.1765\n",
      "Epoch 179/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2188 - val_loss: 0.1829\n",
      "Epoch 180/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2175 - val_loss: 0.1764\n",
      "Epoch 181/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2139 - val_loss: 0.1753\n",
      "Epoch 182/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2167 - val_loss: 0.1790\n",
      "Epoch 183/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2273 - val_loss: 0.1731\n",
      "Epoch 184/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2246 - val_loss: 0.1756\n",
      "Epoch 185/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2218 - val_loss: 0.1718\n",
      "Epoch 186/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2356 - val_loss: 0.1782\n",
      "Epoch 187/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2279 - val_loss: 0.1742\n",
      "Epoch 188/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2152 - val_loss: 0.1756\n",
      "Epoch 189/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2115 - val_loss: 0.1793\n",
      "Epoch 190/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2198 - val_loss: 0.1718\n",
      "Epoch 191/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2168 - val_loss: 0.1743\n",
      "Epoch 192/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2108 - val_loss: 0.1702\n",
      "Epoch 193/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2188 - val_loss: 0.1708\n",
      "Epoch 194/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2275 - val_loss: 0.1725\n",
      "Epoch 195/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2110 - val_loss: 0.1651\n",
      "Epoch 196/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2156 - val_loss: 0.1709\n",
      "Epoch 197/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2199 - val_loss: 0.1700\n",
      "Epoch 198/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2199 - val_loss: 0.1621\n",
      "Epoch 199/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2077 - val_loss: 0.1702\n",
      "Epoch 200/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2152 - val_loss: 0.1702\n",
      "Epoch 201/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2013 - val_loss: 0.1635\n",
      "Epoch 202/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2069 - val_loss: 0.1645\n",
      "Epoch 203/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2249 - val_loss: 0.1720\n",
      "Epoch 204/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2217 - val_loss: 0.1629\n",
      "Epoch 205/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2214 - val_loss: 0.1657\n",
      "Epoch 206/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2195 - val_loss: 0.1624\n",
      "Epoch 207/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2197 - val_loss: 0.1661\n",
      "Epoch 208/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2215 - val_loss: 0.1687\n",
      "Epoch 209/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2227 - val_loss: 0.1698\n",
      "Epoch 210/250\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2136 - val_loss: 0.1713\n",
      "Epoch 211/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2187 - val_loss: 0.1659\n",
      "Epoch 212/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2079 - val_loss: 0.1661\n",
      "Epoch 213/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2120 - val_loss: 0.1646\n",
      "Epoch 214/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2051 - val_loss: 0.1613\n",
      "Epoch 215/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2081 - val_loss: 0.1613\n",
      "Epoch 216/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2059 - val_loss: 0.1616\n",
      "Epoch 217/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2098 - val_loss: 0.1583\n",
      "Epoch 218/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2030 - val_loss: 0.1593\n",
      "Epoch 219/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2222 - val_loss: 0.1632\n",
      "Epoch 220/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2129 - val_loss: 0.1595\n",
      "Epoch 221/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2057 - val_loss: 0.1567\n",
      "Epoch 222/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2066 - val_loss: 0.1553\n",
      "Epoch 223/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2112 - val_loss: 0.1618\n",
      "Epoch 224/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2106 - val_loss: 0.1666\n",
      "Epoch 225/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2094 - val_loss: 0.1630\n",
      "Epoch 226/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2192 - val_loss: 0.1662\n",
      "Epoch 227/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2008 - val_loss: 0.1589\n",
      "Epoch 228/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2111 - val_loss: 0.1615\n",
      "Epoch 229/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2146 - val_loss: 0.1622\n",
      "Epoch 230/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2110 - val_loss: 0.1574\n",
      "Epoch 231/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2124 - val_loss: 0.1592\n",
      "Epoch 232/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2079 - val_loss: 0.1626\n",
      "Epoch 233/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2059 - val_loss: 0.1602\n",
      "Epoch 234/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2004 - val_loss: 0.1635\n",
      "Epoch 235/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2078 - val_loss: 0.1571\n",
      "Epoch 236/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2032 - val_loss: 0.1539\n",
      "Epoch 237/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2251 - val_loss: 0.1597\n",
      "Epoch 238/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2009 - val_loss: 0.1628\n",
      "Epoch 239/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2102 - val_loss: 0.1624\n",
      "Epoch 240/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2134 - val_loss: 0.1533\n",
      "Epoch 241/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2059 - val_loss: 0.1555\n",
      "Epoch 242/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2106 - val_loss: 0.1582\n",
      "Epoch 243/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2138 - val_loss: 0.1562\n",
      "Epoch 244/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2080 - val_loss: 0.1627\n",
      "Epoch 245/250\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2153 - val_loss: 0.1573\n",
      "Epoch 246/250\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.1968 - val_loss: 0.1587\n",
      "Epoch 247/250\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.2058 - val_loss: 0.1599\n",
      "Epoch 248/250\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 0.2081 - val_loss: 0.1569\n",
      "Epoch 249/250\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2191 - val_loss: 0.1510\n",
      "Epoch 250/250\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2022 - val_loss: 0.1570\n"
     ]
    }
   ],
   "source": [
    "history = housing_reg_model.fit(train_set, epochs=250,\n",
    "                                validation_data=valid_set,\n",
    "                                callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
