{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data via TensorFlow\n",
    "\n",
    "Usually working with datasets that will not fit in RAM. Can use the **TensorFlow Dataset API**, which will take care of optimizations including...\n",
    "- Multithreading\n",
    "- Queuing\n",
    "- Batching\n",
    "- Prefetching\n",
    "\n",
    "The Data API helps bringing in from binary, tensorflowbinary, csv, or SQL files/databases, but can also help in its preprocessing.\n",
    "\n",
    "Two things we will focus on:\n",
    "- _TF Transform_ (tf.Transform) - Helps to write a preprocessing function to run in batch mode on the training data such that it can be incorporated into the training model where once it is deployed, it will automatically incorporate new instances.\n",
    "- _TF Datasets_ (TFDS) - Can download many existing datasets and can use database objects for convenient manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## The Data API\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Below is an example that can fit entirely in RAM, but it serves as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from_tensor_slices()** method takes a Tensor and creates a tf.data.Dataset object with elements that are slices of X defaulting to the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Transformations\n",
    "\n",
    "An example of a transformation chain is seen below:\n",
    "\n",
    "![Chaining TensorFlow Transformations](chaining_transformations_tf_dataset.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can even set the drop_remainder flag to True to ensure all the batches have the same size.\n",
    "\n",
    "These dataset methods create new methods, so they need to be saved to a reference. But their elements can also be mutated with the .map method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  0  64 128 192 256 320 384], shape=(7,), dtype=int32)\n",
      "tf.Tensor([448 512 576   0  64 128 192], shape=(7,), dtype=int32)\n",
      "tf.Tensor([256 320 384 448 512 576   0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 64 128 192 256 320 384 448], shape=(7,), dtype=int32)\n",
      "tf.Tensor([512 576], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.map()** method is the one that will be called if your dataset needs preprocessing before being fed into the network. \n",
    "- map(): each item\n",
    "- apply(): dataset as a whole\n",
    "\n",
    "Can also filter the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at just a few items (akin to the Pandas DataFrame .head() method), we can use .take():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "     print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Data\n",
    "\n",
    "Gradient descent works best when the dataset is identically distributed. Can use the .shuffle method to accomplish this. Fills a buffer with a specific number of items in the dataset and randomly pulls them out. Set the buffer to be accomodating for the amount of RAM your computer has. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7, drop_remainder=True)\n",
    "for item in dataset:\n",
    "     print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to split the source data into multiple files and reorder the training database based on the order of the files. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
