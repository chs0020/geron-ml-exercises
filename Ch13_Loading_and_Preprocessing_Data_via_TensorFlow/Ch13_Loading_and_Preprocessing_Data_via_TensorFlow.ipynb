{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data via TensorFlow\n",
    "\n",
    "Usually working with datasets that will not fit in RAM. Can use the **TensorFlow Dataset API**, which will take care of optimizations including...\n",
    "- Multithreading\n",
    "- Queuing\n",
    "- Batching\n",
    "- Prefetching\n",
    "\n",
    "The Data API helps bringing in from binary, tensorflowbinary, csv, or SQL files/databases, but can also help in its preprocessing.\n",
    "\n",
    "Two things we will focus on:\n",
    "- _TF Transform_ (tf.Transform) - Helps to write a preprocessing function to run in batch mode on the training data such that it can be incorporated into the training model where once it is deployed, it will automatically incorporate new instances.\n",
    "- _TF Datasets_ (TFDS) - Can download many existing datasets and can use database objects for convenient manipulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## The Data API\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Below is an example that can fit entirely in RAM, but it serves as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from_tensor_slices()** method takes a Tensor and creates a tf.data.Dataset object with elements that are slices of X defaulting to the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Transformations\n",
    "\n",
    "An example of a transformation chain is seen below:\n",
    "\n",
    "![Chaining TensorFlow Transformations](chaining_transformations_tf_dataset.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can even set the drop_remainder flag to True to ensure all the batches have the same size.\n",
    "\n",
    "These dataset methods create new methods, so they need to be saved to a reference. But their elements can also be mutated with the .map method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.map()** method is the one that will be called if your dataset needs preprocessing before being fed into the network. \n",
    "- map(): each item\n",
    "- apply(): dataset as a whole\n",
    "\n",
    "Can also filter the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at just a few items (akin to the Pandas DataFrame .head() method), we can use .take():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "     print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Data\n",
    "\n",
    "Gradient descent works best when the dataset is identically distributed. Can use the .shuffle method to accomplish this. Fills a buffer with a specific number of items in the dataset and randomly pulls them out. Set the buffer to be accomodating for the amount of RAM your computer has. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7, drop_remainder=True)\n",
    "for item in dataset:\n",
    "     print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to split the source data into multiple files and reorder the training database based on the order of the files. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaving the Data\n",
    "\n",
    "Let's load the California housing dataset as practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"/gv1/users/csmith657/ML Practice Datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = housing.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value  \n",
       "0       322.0       126.0         8.3252            452600.0  \n",
       "1      2401.0      1138.0         8.3014            358500.0  \n",
       "2       496.0       177.0         7.2574            352100.0  \n",
       "3       558.0       219.0         5.6431            341300.0  \n",
       "4       565.0       259.0         3.8462            342200.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book is saying that we are assuming that we have already split the data into a...\n",
    "\n",
    "- training set\n",
    "- validation set\n",
    "- test set\n",
    "\n",
    "and then have split those sets into many .csv files. Essentially, that we have created our own effective batches and saved them as separate .csv files. We haven't done that yet, so let's do it now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can shuffle the DataFrame in-place using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = housing.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's been shuffled, we can then split the data 60/20/20 between the training set, validation set, and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(housing.index)\n",
    "train_rows = int(np.floor(num_rows*0.6))\n",
    "valid_rows = int(np.floor(num_rows*0.2))\n",
    "test_rows = int(np.floor(num_rows*0.2))\n",
    "\n",
    "housing_train = housing.iloc[0:train_rows, :]\n",
    "housing_valid = housing.iloc[train_rows:(train_rows+valid_rows), :]\n",
    "housing_test = housing.iloc[(train_rows+valid_rows):(train_rows+valid_rows+test_rows), :]\n",
    "\n",
    "housing_train.reset_index(drop=True, inplace=True)\n",
    "housing_valid.reset_index(drop=True, inplace=True)\n",
    "housing_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12384, 9)\n",
      "(4128, 9)\n",
      "(4128, 9)\n"
     ]
    }
   ],
   "source": [
    "print(housing_train.shape)\n",
    "print(housing_valid.shape)\n",
    "print(housing_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the separations into DataFrames of the separate sets. We can then iterate over the rows and create a multitude of .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_HOUSING_DIR = os.path.join(\"/gv1/users/csmith657/ML Practice Datasets\", \"housing_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_housing_dat_to_csvs(df, n_csv_files, output_dir, filetitle):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    all_fp = []\n",
    "    \n",
    "    for file in range(n_csv_files):\n",
    "        full_path = os.path.join(output_dir, f\"{filetitle}_{file}.csv\")\n",
    "        df[np.arange(len(df.index))//n_csv_files==file].to_csv(full_path, index=False)\n",
    "        all_fp.append(full_path)\n",
    "        \n",
    "    return all_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = separate_housing_dat_to_csvs(df=housing_train,\n",
    "                                        n_csv_files=50,\n",
    "                                        output_dir=SPLIT_HOUSING_DIR,\n",
    "                                        filetitle=\"my_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fp = separate_housing_dat_to_csvs(df=housing_train,\n",
    "                                        n_csv_files=25,\n",
    "                                        output_dir=SPLIT_HOUSING_DIR,\n",
    "                                        filetitle=\"my_valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp = separate_housing_dat_to_csvs(df=housing_train,\n",
    "                                       n_csv_files=25,\n",
    "                                       output_dir=SPLIT_HOUSING_DIR,\n",
    "                                       filetitle=\"my_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a TensorFlow dataset from these filepaths with the list_files() method, which automatically shuffles the filepaths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp_dataset = tf.data.Dataset.list_files(train_fp, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fp_dataset = tf.data.Dataset.list_files(valid_fp, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp_dataset = tf.data.Dataset.list_files(test_fp, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interleave() method then will read from some integer number of files at a time and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "train_dataset = train_fp_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = valid_fp_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_fp_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at the first rows of five of the .csv files that were taken when the train_dataset was made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-117.72,34.1,52.0,2867.0,496.0,978.0,513.0,3.1477,291200.0'\n",
      "b'-118.47,34.06,45.0,3030.0,433.0,916.0,399.0,9.4664,500001.0'\n",
      "b'-120.72,38.24,32.0,2685.0,543.0,1061.0,492.0,2.5473,101600.0'\n",
      "b'-118.52,34.19,37.0,1560.0,275.0,763.0,284.0,3.8516,206900.0'\n",
      "b'-118.43,34.43,5.0,21113.0,4386.0,9842.0,3886.0,4.2037,194600.0'\n"
     ]
    }
   ],
   "source": [
    "for line in train_dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "\n",
    "Say we want to scale the data as a form of preprocessing. We can find the mean and standard deviation of the columns and scale them as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "X_mean_series = housing.mean(axis=0) \n",
    "X_std_series = housing.std(axis=0)\n",
    "\n",
    "X_mean = np.asarray(X_mean_series.values)\n",
    "X_std = np.asarray(X_std_series.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude               -119.569704\n",
      "latitude                  35.631861\n",
      "housing_median_age        28.639486\n",
      "total_rooms             2635.763081\n",
      "total_bedrooms           537.870553\n",
      "population              1425.476744\n",
      "households               499.539680\n",
      "median_income              3.870671\n",
      "median_house_value    206855.816909\n",
      "dtype: float64\n",
      "[-1.19569704e+02  3.56318614e+01  2.86394864e+01  2.63576308e+03\n",
      "  5.37870553e+02  1.42547674e+03  4.99539680e+02  3.87067100e+00\n",
      "  2.06855817e+05]\n"
     ]
    }
   ],
   "source": [
    "print(X_mean_series)\n",
    "print(X_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude                  2.003532\n",
      "latitude                   2.135952\n",
      "housing_median_age        12.585558\n",
      "total_rooms             2181.615252\n",
      "total_bedrooms           421.385070\n",
      "population              1132.462122\n",
      "households               382.329753\n",
      "median_income              1.899822\n",
      "median_house_value    115395.615874\n",
      "dtype: float64\n",
      "[2.00353172e+00 2.13595240e+00 1.25855576e+01 2.18161525e+03\n",
      " 4.21385070e+02 1.13246212e+03 3.82329753e+02 1.89982172e+00\n",
      " 1.15395616e+05]\n"
     ]
    }
   ],
   "source": [
    "print(X_std_series)\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "  fields = tf.io.decode_csv(line, record_defaults=defs, select_cols=[i for i in range(n_inputs+1)])\n",
    "  x = tf.stack(fields[:-1])\n",
    "  y = tf.stack(fields[-1])\n",
    "  return (x - X_mean[:-1]) / X_std[:-1], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough of the above preprocessing function:\n",
    "- Assuming X_mean and X_std are the mean and standard deviation of the column we want.\n",
    "- The decode_csv() method...\n",
    " - Takes a line and an array of default values\n",
    " - Tells TensorFlow the number of columns and the types of data\n",
    " - Above code says all are floats \n",
    "- decode_csv() method returns a list of scalar tensors (one per column). They are stacked.\n",
    "- Columns are then scaled and returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it Together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=2, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multiple .csv Dataset Preprocessing](multiple_csv_data_pipeline_example.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefetching\n",
    "\n",
    "Prefetching tells TensorFlow to read the data from disk and process it while a training algorithm is working.\n",
    "\n",
    ">> If loading an preprocessing are multithreaded, this is where multiple cores will dramatically improve performance, ensuring the GPU is 100% dedicated to training and the CPU is 100% dedicated to getting the data ready for training.\n",
    "\n",
    "![Why Prefetching is Important for Large Datasets](importance_of_prefetching.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data can fit into RAM:\n",
    "- Use the TensorFlow dataset cache() method to cache it to RAM\n",
    "- Do it after preprocessing\n",
    "- Do it before shuffling, repeating, and batching\n",
    "- That way, each instance is preprocessed only once, and the data will be shuffled differently @ each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **other dataset manipulation methods**:\n",
    "- concatenate()\n",
    "- zip()\n",
    "- window()\n",
    "- reduce()\n",
    "- shard()\n",
    "- flat_map()\n",
    "- padded_batch()\n",
    "\n",
    ">> For **creating your own datasets**, from_generator() or from_tensors() will be very useful\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the Dataset Preprocessing into Keras\n",
    "\n",
    "Now we use the function we just created to bring in our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_fp)\n",
    "valid_set = csv_reader_dataset(valid_fp)\n",
    "test_set = csv_reader_dataset(test_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then build and train a Keras model that will perform on these datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Dataset Processing on Regression Model\n",
    "\n",
    "##### Defining the Model\n",
    "\n",
    "Below is the definition of the model that I will train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a DNN for housing price prediction\n",
    "def build_housing_reg_model(n_hidden=5,\n",
    "                            n_neurons=200,\n",
    "                            learning_rate=0.001,\n",
    "                            beta_1=0.9,\n",
    "                            beta_2=0.999,\n",
    "                            input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden-1):\n",
    "        model.add(keras.layers.Dense(n_neurons,\n",
    "                                     activation=\"elu\",\n",
    "                                     kernel_initializer=\"he_normal\"))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.Dropout(rate=0.2))\n",
    "    model.add(keras.layers.Dense(n_neurons, activation=\"elu\"))\n",
    "    model.add(keras.layers.Dense(1, activation=\"elu\"))\n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the TensorBoard to Visualize its Training\n",
    "\n",
    "To use the TensorBoard, the model needs to output binary files called _event files_, a grouping of which constitutes a record or a _summary_. TensorBoard monitors the log directory and updates its visualization based on the files in there. \n",
    "\n",
    "First, we define the root log directory where we will organize the summaries of different runs, organized in their own subdirectories labeled with each run's date and time such that we can compare different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to create log directory for the\n",
    "# runs at various datetimes\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with the early stopping callback to ensure we aren't overtraining, we can callback to the TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TensorBoard callback\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to start the TensorBoard server, we can run calls directly within Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the Regression Model\n",
    "\n",
    "Let's build it and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_reg_model = build_housing_reg_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 57888729010.2279 - val_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 57457019144.0105 - val_loss: 55695436902.4000\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 55639074631.8355 - val_loss: 54383841689.6000\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 55320470757.0649 - val_loss: 52203655987.2000\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 52416319820.2426 - val_loss: 49178786406.4000\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 49669247144.4026 - val_loss: 45579362201.6000\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 46086110615.2150 - val_loss: 40338434662.4000\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 40723059966.5742 - val_loss: 35161245081.6000\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 34662453772.4954 - val_loss: 29222053785.6000\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 29505358650.4587 - val_loss: 23404839321.6000\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 23217607890.1922 - val_loss: 18820844851.2000\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 18393077069.4870 - val_loss: 13090097510.4000\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 13984766965.3193 - val_loss: 10453091660.8000\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 11613068982.2461 - val_loss: 7335028236.8000\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 9126738870.0128 - val_loss: 6301603046.4000\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 7871262817.9929 - val_loss: 5381735846.4000\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 7376634147.1271 - val_loss: 5061613747.2000\n",
      "Epoch 18/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 6742348379.9008 - val_loss: 4626274195.2000\n",
      "Epoch 19/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 6300243933.1840 - val_loss: 4529248025.6000\n",
      "Epoch 20/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 5824703325.8710 - val_loss: 4274708582.4000\n",
      "Epoch 21/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 5598504931.7169 - val_loss: 4456588569.6000\n",
      "Epoch 22/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 5350953200.9900 - val_loss: 4170545920.0000\n",
      "Epoch 23/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 5282774528.7259 - val_loss: 4188211942.4000\n",
      "Epoch 24/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 5017020166.4810 - val_loss: 3825595468.8000\n",
      "Epoch 25/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 5147120207.6646 - val_loss: 3883393574.4000\n",
      "Epoch 26/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4768421724.5748 - val_loss: 3982694268.8000\n",
      "Epoch 27/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4719785394.3056 - val_loss: 3928980627.2000\n",
      "Epoch 28/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4811340490.0132 - val_loss: 4096062969.6000\n",
      "Epoch 29/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4801729019.6448 - val_loss: 3786532284.8000\n",
      "Epoch 30/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4593464501.2091 - val_loss: 4061299705.6000\n",
      "Epoch 31/100\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 4804309673.1285 - val_loss: 3837360019.2000\n",
      "Epoch 32/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4796223721.3942 - val_loss: 3947842553.6000\n",
      "Epoch 33/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4587951845.8945 - val_loss: 3640877830.4000\n",
      "Epoch 34/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4469124859.5929 - val_loss: 3719078515.2000\n",
      "Epoch 35/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4594779520.2981 - val_loss: 3549984864.0000\n",
      "Epoch 36/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4803984995.9631 - val_loss: 3553964761.6000\n",
      "Epoch 37/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4634888358.2509 - val_loss: 3491943865.6000\n",
      "Epoch 38/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 4522545076.7554 - val_loss: 3342811532.8000\n",
      "Epoch 39/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4463329530.7633 - val_loss: 3383615200.0000\n",
      "Epoch 40/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4654539918.0119 - val_loss: 3362585964.8000\n",
      "Epoch 41/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4175146838.6641 - val_loss: 3581431244.8000\n",
      "Epoch 42/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4333855477.0341 - val_loss: 3216831782.4000\n",
      "Epoch 43/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4303558738.3607 - val_loss: 3337393843.2000\n",
      "Epoch 44/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 4224458429.9326 - val_loss: 3427723673.6000\n",
      "Epoch 45/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4091936379.0226 - val_loss: 3250913350.4000\n",
      "Epoch 46/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4208438443.9283 - val_loss: 3120499462.4000\n",
      "Epoch 47/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4458565543.8842 - val_loss: 3154238348.8000\n",
      "Epoch 48/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4040786169.8819 - val_loss: 3097374208.0000\n",
      "Epoch 49/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4089742058.6515 - val_loss: 3059099840.0000\n",
      "Epoch 50/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4015420442.0666 - val_loss: 3050320083.2000\n",
      "Epoch 51/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 3920891665.9265 - val_loss: 3000939206.4000\n",
      "Epoch 52/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4139878965.3258 - val_loss: 2983647792.0000\n",
      "Epoch 53/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3947906111.7861 - val_loss: 3064996992.0000\n",
      "Epoch 54/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3943515284.7392 - val_loss: 3100690112.0000\n",
      "Epoch 55/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3968197665.1309 - val_loss: 2988370784.0000\n",
      "Epoch 56/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3642424984.7315 - val_loss: 2958766528.0000\n",
      "Epoch 57/100\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 3928177738.4539 - val_loss: 2983166931.2000\n",
      "Epoch 58/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4025621351.5407 - val_loss: 3090532755.2000\n",
      "Epoch 59/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3857857495.5326 - val_loss: 2839448134.4000\n",
      "Epoch 60/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3876433647.5123 - val_loss: 2937417996.8000\n",
      "Epoch 61/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4007707819.0339 - val_loss: 2939688249.6000\n",
      "Epoch 62/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3919744965.7098 - val_loss: 2907445267.2000\n",
      "Epoch 63/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3744783203.8854 - val_loss: 2935412838.4000\n",
      "Epoch 64/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 3813062246.4518 - val_loss: 2986767225.6000\n",
      "Epoch 65/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3902348251.1749 - val_loss: 2835764467.2000\n",
      "Epoch 66/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3991161256.4545 - val_loss: 2945056864.0000\n",
      "Epoch 67/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3638934676.7911 - val_loss: 2844267872.0000\n",
      "Epoch 68/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 4030265003.1247 - val_loss: 2930442758.4000\n",
      "Epoch 69/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3836775072.1847 - val_loss: 2833602048.0000\n",
      "Epoch 70/100\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 3930456464.4488 - val_loss: 2825879110.4000\n",
      "Epoch 71/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3826708493.2990 - val_loss: 2940173318.4000\n",
      "Epoch 72/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3912183932.4354 - val_loss: 2942551084.8000\n",
      "Epoch 73/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3865582536.1466 - val_loss: 2948766393.6000\n",
      "Epoch 74/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3922453310.3733 - val_loss: 2791106252.8000\n",
      "Epoch 75/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3825177209.7912 - val_loss: 2987177152.0000\n",
      "Epoch 76/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3751927150.3328 - val_loss: 2859090704.0000\n",
      "Epoch 77/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 3953528818.2862 - val_loss: 2785315942.4000\n",
      "Epoch 78/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3625892822.5474 - val_loss: 2774749792.0000\n",
      "Epoch 79/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3432658494.0363 - val_loss: 2900132640.0000\n",
      "Epoch 80/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 3636038815.7440 - val_loss: 2774992448.0000\n",
      "Epoch 81/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3675947813.0325 - val_loss: 2762351872.0000\n",
      "Epoch 82/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3625923223.2409 - val_loss: 2790231686.4000\n",
      "Epoch 83/100\n",
      "79/79 [==============================] - 1s 12ms/step - loss: 3883178645.6336 - val_loss: 2773304345.6000\n",
      "Epoch 84/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3835538346.1914 - val_loss: 2830508224.0000\n",
      "Epoch 85/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3683421430.1359 - val_loss: 2933687014.4000\n",
      "Epoch 86/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3854139235.7946 - val_loss: 2846296467.2000\n",
      "Epoch 87/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3638589113.1755 - val_loss: 2688643078.4000\n",
      "Epoch 88/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3466439126.5734 - val_loss: 2795791062.4000\n",
      "Epoch 89/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3995624599.0076 - val_loss: 2885112019.2000\n",
      "Epoch 90/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 3739150906.1995 - val_loss: 2721620486.4000\n",
      "Epoch 91/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3559438254.7022 - val_loss: 2841133126.4000\n",
      "Epoch 92/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3584147842.1776 - val_loss: 2760475033.6000\n",
      "Epoch 93/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3568525095.8712 - val_loss: 2670537504.0000\n",
      "Epoch 94/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3614349139.7217 - val_loss: 2728847123.2000\n",
      "Epoch 95/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3638519926.2655 - val_loss: 2727257504.0000\n",
      "Epoch 96/100\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 3622197468.5877 - val_loss: 2613130848.0000\n",
      "Epoch 97/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3835785830.1926 - val_loss: 2601148403.2000\n",
      "Epoch 98/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3898948959.5301 - val_loss: 2798053740.8000\n",
      "Epoch 99/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3460194340.9418 - val_loss: 2687148630.4000\n",
      "Epoch 100/100\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 3778167324.5294 - val_loss: 2618192947.2000\n"
     ]
    }
   ],
   "source": [
    "history = housing_reg_model.fit(train_set, epochs=100,\n",
    "                                validation_data=valid_set,\n",
    "                                callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: save a JSON file of the model parameters in the TensorBoard logfiles.\n",
    "\n",
    "Based on [this StackOverflow threat](https://datascience.stackexchange.com/questions/19578/why-my-training-and-validation-loss-is-not-changing), we have encountered the dying ReLU problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
