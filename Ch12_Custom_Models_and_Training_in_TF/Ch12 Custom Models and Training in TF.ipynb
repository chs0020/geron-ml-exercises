{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Quick Overview of TensorFlow\n",
    "\n",
    "High level summary:\n",
    "- Highly efficient computationaly library (like NumPy)\n",
    "- Can optimize via JIT (just-in-time) compiler by creating a \"computation graph\" \n",
    "- Export in one environment (Python in Linux) and run it in another (Java on Android)\n",
    "- Lowest level is in hyperefficient C++ code\n",
    "- Distribute computations across multiple devices and servers\n",
    "\n",
    "Some important features/packages:\n",
    "- AI front-end... tf.keras\n",
    "- Data loading and pre-processing... tf.data, tf.io\n",
    "- Image processing... tf.image\n",
    "- Signal processing... tf.signal\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Using TensorFlow like Numpy\n",
    "\n",
    "**Tensor**: Multidimensional array (like [ndarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html)). Could also hold a scalar.\n",
    "\n",
    "***\n",
    "## Tensors and Operations\n",
    "\n",
    "We can create **constant** tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [3. 4. 5.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tfmatrixexample = tf.constant([[1.0, 2.0, 3.0],[3.0, 4.0, 5.0]])\n",
    "print(tfmatrixexample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([42], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tfscalarexample = tf.constant([42])\n",
    "print(tfscalarexample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **index** tensors like we can with NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [4., 5.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmatrixexample[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9, shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmatrixexample[-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=13, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [4.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmatrixexample[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform **tensor operations** with some simple calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=15, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [13., 14., 15.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmatrixexample + 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=16, shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [ 9., 16., 25.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(tfmatrixexample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **@** symbol for matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=19, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 26.],\n",
       "       [26., 50.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmatrixexample @ tf.transpose(tfmatrixexample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow and NumPy Interchange\n",
    "\n",
    "TensorFlow uses 32-bit precision (to save on computation efficiency), since often that's precise enough. NumPy uses 64-bit precision. So, when playing the two together, set the numpy precision:\n",
    "\n",
    "> dtype=float32\n",
    "\n",
    "Some interchangable operations are seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=20, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [3., 4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmatrixexample.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=22, shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [ 9., 16., 25.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(tfmatrixexample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=24, shape=(6,), dtype=float32, numpy=\n",
       "array([0.       , 4.       , 0.       , 5.0625   , 0.       , 5.3476562],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype = np.dtype(np.float32)\n",
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Conversions\n",
    "\n",
    "Computationally expensive. Avoid it. They're done automatically in NumPy, but NOT IN TensorFlow. See the error below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0e485bc98398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tfkeraspractice/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfkeraspractice/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tfkeraspractice/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    544\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
      "\u001b[0;32m~/.conda/envs/tfkeraspractice/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/"
     ]
    }
   ],
   "source": [
    "tf.constant(2.0) + tf.constant(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Variables\n",
    "\n",
    "tf.**constant**: Immutable, so not good for a weight structure.\n",
    "\n",
    "tf.**Variable**: Mutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvar_example = tf.Variable([[1., 2., 3.], [3., 4., 5.]])\n",
    "print(tfvar_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the values or slices in place with the **.assign()** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfvar_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1ab296b5701e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfvar_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtfvar_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfvar_example' is not defined"
     ]
    }
   ],
   "source": [
    "tfvar_example.assign(2 * tfvar_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfvar_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d3d9edfb1aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfvar_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfvar_example' is not defined"
     ]
    }
   ],
   "source": [
    "tfvar_example[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfvar_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9ee111d895de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfvar_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfvar_example' is not defined"
     ]
    }
   ],
   "source": [
    "tfvar_example[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfvar_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-64952bee9ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfvar_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_nd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tfvar_example' is not defined"
     ]
    }
   ],
   "source": [
    "tfvar_example.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Data Structures\n",
    "\n",
    "| Data Structure | TensorFlow | Description |\n",
    "| --- | --- | --- |\n",
    "| Spare Tensor | tf.SpareTensor | Tensor that contains mostly zeroes. |\n",
    "| Tensor Array | tf.TensorArray | List of Tensors of the same shape and data type. Length can be made dynamic. |\n",
    "| Ragged Tensor | tf.RaggedTensor | Static list of lists of Tensors of same shape and data types. |\n",
    "| String Tensor | tf.string | Byte strings (encoded in utf8). Can convert to Unicode. |\n",
    "| Sets | tf.sets package |  |\n",
    "| Queues | tf.queue package | Implementing FIFO, Random, Priority, etc. queues of Tensors. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Customizing Models and Training Algorithms\n",
    "\n",
    "Often, can be defined as a...\n",
    "\n",
    "- function (def func():)\n",
    "- class (class MyFunc(): )\n",
    "\n",
    "The rule of thumb is that when a hyperparameter or aspect of the class needs to be saved to the model, the class implementation is probably the way to go. Plus, it looks fancier.\n",
    "\n",
    "***\n",
    "## Custom Implementations - Loss Functions\n",
    "\n",
    "Pretending the Huber Loss function is not available in...\n",
    "\n",
    "> keras.losses.Huber\n",
    "\n",
    "... we can create a function that takes labels and predictions as arguments and uses Tensors to compute the Huber loss function. Keep a few things in mind:\n",
    "\n",
    "- Use TensorFlow operations to take advantage of graph features\n",
    "- Vectorize inputs when you can\n",
    "- Preferable to return Tensor w/ one loss per instance\n",
    "\n",
    "Implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example on how to call this new loss function is seen with the line below...\n",
    "\n",
    "> model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Saving Models that Contain Custom Components\n",
    "\n",
    "- When saving the model, the model will save the _name_ of the custom function. \n",
    "- When loading the model, provide a dict that maps the function name to the function definition.\n",
    "\n",
    "An example is seen below:\n",
    "\n",
    "> model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    ">                                 custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Hyperparameters to the Custom Function\n",
    "\n",
    "#### Via Nested Methods\n",
    "\n",
    "If we want to pass something to this loss function, it is a good idea to encapsulate that loss function definition in an initializing function. For example, if we wanted to change the threshold of the Huber function, we can do so like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    \"\"\"Huber loss function initializer.\n",
    "    \"\"\"\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it can be called when fitting to data as...\n",
    "\n",
    "> model.compile(loss=create_huber(threshold=2.0), optimizer=\"nadam\")\n",
    "\n",
    "When loading this sort of function, the hyperparameter is not going to be saved, so it has to be passed again. Also, what Keras saves is the nested function inside the \"constructor\", which in the case above is the \"huber_fn\". So, loading a model that was built with the \"create_huber\" function as the loss function would be done as...\n",
    "\n",
    "> model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
    ">                                 custom_objects={\"huber_fn\": create_huber(2.0)})\n",
    "\n",
    "#### Via Class Definition\n",
    "\n",
    "Keras API supports custom class definition for... \n",
    "\n",
    "- layers\n",
    "- models\n",
    "- callbacks\n",
    "- regularizers\n",
    "\n",
    "Other components such as...\n",
    "\n",
    "- losses\n",
    "- metrics\n",
    "- initializers\n",
    "- constraints\n",
    "\n",
    "may not port to other Keras impleentations. But, it can still be useful for your targeted tasks. \n",
    "\n",
    "The reason it can still be useful is it can solve the problem of saving the configuration (including the hyperparameters you use) by creating a custom \"keras.losses.Loss\" class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    \"\"\"Custom HuberLoss class.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __init__:\n",
    " - Acceps \"**kwargs\" to pass to parent constructor (Keras.losses.loss.__init__(**kwargs)). Includes all the args [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss)\n",
    " \n",
    "- call:\n",
    " - Takes labels, predictions, computes instance losses, returns them\n",
    " \n",
    "- get_config:\n",
    " - Maps hyperparameter name to value and can add whatever other hyperparameter your custom function uses, which in this case is \"threshold\".\n",
    " \n",
    " Use the above class definition as...\n",
    " \n",
    " > model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
    " \n",
    " Load the model that uses the above class as...\n",
    " \n",
    " > model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
    " >                                 custom_objects={\"HuberLoss\": HuberLoss})\n",
    " \n",
    " Saving the model will automatically call the loss instance's \"get_config()\" method and save the configuration as JSON in HDF5 file. Loading the model grabs that info and passes it to the class constructor (when the class is defined during the load, I am pretty sure). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Custom Components - Activation Functions, Initializers, Regularizers, and Constraints\n",
    "\n",
    "Examples seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments clearly depend on the type of function. See what arguments are necessary for which custom function.\n",
    "\n",
    "Use them as...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes...\n",
    "- Activation function (\"my_softplus()\") applied to output of the Dense layer; result is passed to the next layer\n",
    "- Weights initialized with the value returned by the initializer (\"my_glorot_initializer()\")\n",
    "- @ each training step, weights are passed to the regularizer (\"my_l1_regularizer()\") and the output is added to the loss function\n",
    "- Contraint function (\"my_positive_weights()\") is called after each training step to adjust weights falling outside of the constraint\n",
    "\n",
    "To create custom classes, have your class inherit...\n",
    "- Initializer: \"keras.initializers.Initializer\"\n",
    "- Regularizer: \"keras.regularizers.Regularizer\"\n",
    "- Constraint: \"keras.constraints.Constraint\"\n",
    "- Arbitrary Layer (including activation functions): \"keras.layers.Layer\"\n",
    "\n",
    "Again, classes can save and retrieve hyperparameters when a model is saved. An example is seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note...\n",
    "\n",
    "- Use \"\\_\\_call__():\" for...\n",
    " - initializers\n",
    " - regularizers\n",
    " - constraints\n",
    " \n",
    "- Use \"call():\" for..\n",
    " - losses\n",
    " - arbitrary layers (including activation functions)\n",
    " - models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Custom Components - Metrics\n",
    "\n",
    "Losses and metrics are not the same. Losses need to be differentiable so that the model can be trained. Metrics have fewer constraints (for instance, don't have to be differentiable for Gradient Descent, like losses do) and are used to evaluate the final product. \n",
    "\n",
    "That said, creation is a similar process. Could even use the Huber Loss function we defined above as a metric:\n",
    "\n",
    "> model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "\n",
    "Some notes on this:\n",
    "- Keeps track of its mean since beginning of the epoch, which is good but not always what we need.\n",
    "- If we want to keep track of the number of true positives and false positives, however, other than just a total running average of the precision, we can use...\n",
    "\n",
    "\n",
    "> keras.metrics.Precision()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=93, shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of using the Precision class\n",
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=141, shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the above is of a _streaming metric_, where the precision (numpy=__ field in the output) is updated batch after batch, not just averaging the precisions of batches together. It's a subtle difference.\n",
    "\n",
    "We can see the result of the current metric, get its stored variables, or reset its state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=151, shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=167, shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a custom _streaming metric_, inheret the Keras class...\n",
    "\n",
    "> keras.metrics.Metric \n",
    "\n",
    "An example for the Huber Loss can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough:\n",
    "- self.add_weight() - used to keep track of any variables (sum of all Huber losses in self.total, and number of instances seen so far in self.count). That's why it's in the \\_\\_init_\\_ constructor.\n",
    "- update_state(*args) -  method that is called when instance of the class is called as a function (did this with the Precision() class)\n",
    "- result() - computes and returns final result (but update_state() is called first when the metric is called as a function)\n",
    "- get_config() - method to save any hyperparameters (in this case the threshold as a member of the JSON dict, which again, gets passed into the constructor when it is loaded in)\n",
    "- reset_states() - inherited in the example above, but defaults to setting all instance vars to 0.\n",
    "\n",
    "Any metric that can't be averaged over a batch, a class implementation is best (or required). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Custom Components - Layers\n",
    "\n",
    "Will need this if...\n",
    "- using an architecture with a very contemporary and exotic layer\n",
    "- for convenience with repetative architectures (A B C A B C A B C A B C to D D D D where $D = A, B, C$)\n",
    "\n",
    "### Layers with No Weights\n",
    "\n",
    "Examples we've seen:\n",
    "- keras.layers.Flatten\n",
    "- keras.layers.ReLU\n",
    "\n",
    "Solution:\n",
    "- Write a function.\n",
    "- Wrap it in keras.layers.Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the Keras Lambda layer for layers \n",
    "# without weights.\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use these layers as activation functions (exponential used often if regression models are expected to have logarithmic outputs, maybe if something were to try and predict an RF ouptut power, or any other sorts of signals measured in logarithmic units like dB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers with Weights\n",
    "\n",
    "Create a subclass of...\n",
    "\n",
    "> keras.layers.Layer\n",
    "\n",
    "A simplified version of the Dense layer is seen below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    \"\"\"Simplified version of Keras Dense layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough:\n",
    "- **\\_\\_init__**: takes hyperparameters (example above, units and activation) as inputs along with the special kwargs dict, which takes care of any other things you can pass to the parent class, which is the keras.layers.Layer object.\n",
    "- **build**: Called when the layer is first used. It is used to create layer's variables, which includes a weight for each needed weight. Keras passes the shape of the layer inputs to the build method to know how many weights are needed (connection weights matrix known as the \"kernel\"). Must call the parent's build method to let Keras know the layer has been build.\n",
    "- **call**: performs the different operations needed for that layer. In this case, it's the activation function. The layer could do other things, like 2D convolution, or filtering, etc.\n",
    "- **compute_output_shape**: returns shape of layer outputs. \n",
    "- **get_config**: used just like other layers to see the parameters of the layer\n",
    "\n",
    "Multiple inputs to a layer (like Concatenate) require special attention:\n",
    "- **call** and **compute_output_shape** input arguments requires a tuple containing all inputs\n",
    "- Recall that this must be used with Functional or Subclassing Keras API (Sequential takes one input and one output).\n",
    "\n",
    "Example of this can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1] # should probably handle broadcasting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different behavior between training and testing (like with some RNNs or with Dropout/Batch Normalization):\n",
    "- Add training argument to **call** method. \n",
    "- Example is a layer to add Gaussian noise during training but not during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Custom Components - Models\n",
    "\n",
    "### Subclassing API\n",
    "\n",
    "Just like above, we will create our own class that inherits the keras.Model parent class.\n",
    "\n",
    "![Custom Model](custom_model_tensorflow.PNG)\n",
    "\n",
    "We'll give an example of building a small block of a model and then connecting the entire thing together. The entire model has a few small blocks, residual blocks, where the data will travel through a layer three times before heading to the next one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **ResidualBlock** class, Keras checks for the self.hidden attribute. \n",
    "\n",
    "\n",
    "Using the **Subclassing API**, we can define the model itself. The constructor makes the layers themselves. The layers are then used in the call method. The model is using Z as the output/input during the call method. It is being fed through the first ResidualBlock 3 times before it is moved on to the second. Must implement **get_config** in both ResidualBlock and the ResidualRegressor if we want to use the keras.models.save() or use the keras.models.load_model() functions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Metrics Based on Model Internals\n",
    "\n",
    "Some losses are not going to be dependent on just the predictions or the outputs. Some losses need to be defined by the weights or the activations of hidden layers.\n",
    "\n",
    "These types of losses can be created using the inherited self.add_loss() method. \n",
    "\n",
    "Example below creates a model with..\n",
    "- 5 hidden layers\n",
    "- 1 regular output layer and 1 aux output layer\n",
    "- Loss will be associated with aux output layer \n",
    " - **reconstruction loss**, used in generative modeling\n",
    " - Mean Square Difference b/w reconstruction and inputs \n",
    " - Encourages preserving info through the layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough:\n",
    "- **__init__**: Creates 5 hidden layers, 30 neurons each, with 1 output layer\n",
    "- **build**: Creates extra dense layer to reconstruct inputs. Created with build so that it can match the output to the number of inputs, which is unknown until build is called.\n",
    "- **call**: \n",
    " - Process input through all 5 hidden layers and passes data through the reconstruction layer.\n",
    " - Computes reconstruction loss and adds it to model's list of losses with add_loss() method, but scaled down by a tunable hyperparameter\n",
    " - Passed output of hidden layers to output layer and return the output\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom metrics:\n",
    "- Create an object that inherits from or is an instance of a keras.metrics object in the **__init__** method. \n",
    "- Call that metric in the **call** method, passing it whatever aspects of the model is needed to compute said metric.\n",
    "- Add it to the model with the inherited self.add_metric() method such that Keras will display said metric during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients using Autodiff\n",
    "\n",
    "Consider the equation below:\n",
    "\n",
    "$$ f(w_1 , w_2) = 3w_{1}^{2} + 2w_1 w_2 $$\n",
    "\n",
    "We can analytically perform these partial derivatives quickly:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial w_1} = 6w_1 + 2w_2 $$\n",
    "\n",
    "$$  \\frac{\\partial f}{\\partial w_1} = 3w_{1}^{2} + 2w_1 $$\n",
    "\n",
    "And then we can immediately evaluate these partial derivatives given the certain parameters $w_1$ or $w_2$. But neural networks span a much larger and much more complex parameter space. \n",
    "\n",
    "To better navigate this space, TensorFlow uses **autodiff**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=209, shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: id=201, shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling gradients more than once results in a runtime error. If it has to be done, make an instance of the tf.GradientTape object, then delete it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\n",
    "dz_dw2 = tape.gradient(z, w2) # => tensor 10.0, works fine now!\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have the gradients watch any sort of parameter outside of just the variables, use the code below, which involves explicit watch commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful in the case where you want to penalize something like the activations that vary significantly given an input. \n",
    "\n",
    "Important notes:\n",
    "- Typically, just taking the gradient of one value w/ repsect to other parameters\n",
    " - Grad of loss with respect to model parameters\n",
    "- Reverse-mode autodiff\n",
    " - Gets all gradients w/ a forward and reverse pass\n",
    "- Can compute the **Jacobian** (elementwise gradients of the loss of a vector rather than the gradient of the sum) and can compute the **Hessians** or second derivatives, if needed.\n",
    "\n",
    "Can also not let a gradient through during backpropagation using tf.stop_gradient() (it does not perform any gradient on the input on the forward pass and treats the input like a constant on the backwards pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2) # same result as without stop_gradient()\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever get into an issue where the gradients are returning NaNs due to precision errors or the like, can define custom gradients. We do this for a softplus function below as an example. The custom output returns both the regular function and the custom function that computes its gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Training Loops\n",
    "\n",
    "Defer to the Keras fit() method, as custom training loops make for harder code maintenance. However, for a few different papers (say, a network whose paths use two different optimizers), a custom training loop is needed.\n",
    "\n",
    "Below is an example network we will use to build the custom loop around.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define custom functions to...\n",
    "- randomly sample a batch of instances from the training data (though TensorFlow Data API has a function that will do the same thing)\n",
    "- display the training status with various custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these custom functions, we can define the custom training loop. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train, y_train = mnist[0]\n",
    "X_test, y_test = mnist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling for Grad Descent and splitting into validation set\n",
    "X_train_scaled = X_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "60000/60000 - mean: 8.7057 - mean_absolute_error: 2.5428\n",
      "60000/60000 - mean: 8.7057 - mean_absolute_error: 2.5428\n",
      "Epoch 2/5\n",
      "60000/60000 - mean: 8.4753 - mean_absolute_error: 2.5338\n",
      "60000/60000 - mean: 8.4753 - mean_absolute_error: 2.5338\n",
      "Epoch 3/5\n",
      "60000/60000 - mean: 8.4229 - mean_absolute_error: 2.5312\n",
      "60000/60000 - mean: 8.4229 - mean_absolute_error: 2.5312\n",
      "Epoch 4/5\n",
      "60000/60000 - mean: 8.3815 - mean_absolute_error: 2.5265\n",
      "60000/60000 - mean: 8.3815 - mean_absolute_error: 2.5265\n",
      "Epoch 5/5\n",
      "60000/60000 - mean: 8.4046 - mean_absolute_error: 2.5292\n",
      "60000/60000 - mean: 8.4046 - mean_absolute_error: 2.5292\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough:\n",
    "- 1 loop for epochs, 1 loop for batches within the epochs\n",
    "- Sample a random batch from the training set\n",
    "- tf.GradientTape() block makes a prediction for one batch, computes the loss over the entire batch using reduce_mean(), which can apply different weights for each instance as opposed to each batch, and the regularization loss gets summed with this loss.\n",
    "- Compute gradient of loss with respect to each trainable variable\n",
    "- Update mean loss, the metrics, and display status bar\n",
    "- Update status bar at end of each epoch\n",
    "\n",
    "To add constraints to the model weights, add a conditional loop just after the apply_weights() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in model.variables:\n",
    "    if variable.constraint is not None:\n",
    "        variable.assign(variable.constraint(variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle training and testing differently, like we do during BatchNormalization or Dropout, call the model with training=True and propagate this to each layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## TensorFlow Functions and Graphs\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Starting with a simple function:\n",
    "\n",
    "$$ f(x) = x^3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can call this with either a Pythonic variable or a TensorFlow variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2738037, shape=(), dtype=float32, numpy=27.0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a decorator to make this a TensorFlow function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF functions will simplify expressions where it can, and run the operations in parallel when it can. It will almost always run faster, so run these when you can.\n",
    "\n",
    ">> Do NOT pass Pythonic variables to these TensorFlow functions unless they are not going to vary much or there are not many of them. A new graph is made for each instance call, so reserve these for stuff like hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograph and Tracing\n",
    "\n",
    "TensorFlow graph creation process:\n",
    "\n",
    "1. Analyze all control statements\n",
    " - for\n",
    " - while\n",
    " - if, elif, else\n",
    " - break\n",
    " - continue\n",
    " - return\n",
    "    and call _AutoGraph_. \n",
    "    \n",
    "2. Outputs an upgraded version of said function replacing those control statements with their TensorFlow operative equivalents (tf.while_loop(), tf.cond(), etc.).\n",
    "\n",
    "![Example of TensorFlow Graph Creation](tensorflow_graph_creation.PNG)\n",
    "\n",
    "3. Passes a \"symbolic tensor\" such that each TensorFlow operation will add a node to the graph to represent itself and its outputs. Nodes represent operations, and arrows represent Tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Function Rules\n",
    "\n",
    "- Regarding the @tf.function decorator and external libraries:\n",
    " - TensorFlow graphs can only use TensorFlow constructs, so use tf.reduce_sum() instead of np.sum(), tf.sort() instead of a built-in sorted(), etc.\n",
    " - If the TensorFlow function is updating other Pythonic elements, like a counter or something, those will only be updated when the graph is traced, not every time the graph is ran. So try to compartmentalize these TensorFlow functions.\n",
    "- Can call other Python or TensorFlow functions (they need not have the @tf.function decorator), and their operations will be captured in the graph, but they require the same rules\n",
    "- If a function is creating a TensorFlow variable or object, must do so only on very first call, or else it will throw an error. Do these creations outside of TensorFlow Functions (such as in the build method of a custom layer, and then use the assign method in the function).\n",
    "- For loop iterations over the Python range() function will not be captured in the graph, but tf.range() will be.\n",
    "- Take preference over vectorized implementations rather than iterative loops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
