{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 4: Training Models\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "What did we do last few chapters? \n",
    " - Optimized a regression system\n",
    " - Improved image classification\n",
    " \n",
    "Time to look under the hood for more efficient debugging.\n",
    "\n",
    "What will we look at upcoming?\n",
    " - Linear Regression models\n",
    "  - Training Method 1: \"closed-form\" equation computing best fit model parameters to minimize cost function over training set\n",
    "  - Training Method 2: \"Gradient Descent\", tweaking parameters to converge on model parameters that minimize cost function.\n",
    " - Polynomial Regression models\n",
    "  - Detect overfitting\n",
    "  - Regularization techniques to avoid overfitting\n",
    "  - Regression for classification\n",
    "   - Logistic regression\n",
    "   - Softmax regression\n",
    "***\n",
    "\n",
    "## 4.1 - Linear Regression\n",
    "\n",
    "### 4.1.1 - Mathematical Definition of Linear Regression Model\n",
    "\n",
    "Linear model predicts via a weighted sum of input features plus a bias term:\n",
    "\n",
    "$$\\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n}$$\n",
    "\n",
    "- $\\hat{y}$: predicted value\n",
    "- $n$: number of features (or dimensions)\n",
    "- $x_{i}$: ith feature\n",
    "- $\\theta_{j}$: jth model parameter including bias term as $\\theta_{0}$\n",
    "\n",
    "In vectorized notation:\n",
    "\n",
    "$$\\boxed{\\hat{y} = h_{\\boldsymbol{\\theta}} \\textbf{(x)} = \\boldsymbol{\\theta} \\cdot \\textbf{x}}$$\n",
    "- $\\boldsymbol{\\theta}$: **Parameter Vector** containing bias term $\\theta_{0}$ and feature weights $\\theta_1$ to $\\theta_n$\n",
    "- $\\textbf{x}$: **Feature Vector** containing $x_0$ to $x_n$ where $x_0 = 1$\n",
    "- $\\boldsymbol{\\theta} \\cdot \\textbf{x}$: **Dot Product** of _parameter_ and _feature_ vectors (element-wise multiplaction then summing)\n",
    "- $h_0$: **Hypothesis Function** using model parameters $\\boldsymbol{\\theta}$\n",
    "\n",
    "Vectors are often represented as **column vectors**. If we are indeed only dealing with column vectors of a single column for a single instance (and we include our bias term), we can say our prediction in matrix multiplication form becomes:\n",
    "\n",
    "$$\\boxed{\\hat{y} = \\boldsymbol{\\theta}^{T}\\textbf{x}}$$\n",
    "\n",
    "### 4.1.2 - Training Linear Regression: Cost Function\n",
    "\n",
    "Ch. 2, we saw common use is the _Root Mean Square Error_ (RMSE). We want to find a value of $\\boldsymbol{\\theta}$ that minimizes the RMSE. Can also use the _Mean Square Error_ (MSE) as if the square root of a function is minimized, so is the function.\n",
    "\n",
    "**MSE of Linear Regression of hypothesis** $h_{\\theta}$ **on a training set** $\\textbf{X}$:\n",
    "\n",
    "$$\\boxed{MSE(\\textbf{X}, h_{\\theta}) = \\frac{1}{m} \\sum_{i=1}^{m} (\\boldsymbol{\\theta}^{T}\\textbf{x}^{(i)} - y^{(i)})^2}$$\n",
    "\n",
    "### 4.1.3 - Training Linear Regression: Closed Form Solution of Minimization of Cost Function\n",
    "\n",
    "There is an equation that directly gives the best value for $\\boldsymbol{\\theta}$ called the **normal equation**:\n",
    "\n",
    "$$\\boxed{\\boldsymbol{\\hat{\\theta}} = (\\textbf{X}^{T}\\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}}$$\n",
    "\n",
    "- $\\hat{\\boldsymbol{\\theta}}$: parameter vector that minimizes cost function\n",
    "- $\\textbf{y}$: vector of target values\n",
    "\n",
    "#### 4.1.3.1 - Testing and Plotting Closed Form Solution\n",
    "\n",
    "Below, we can generate a random dataset with a linear dependence and compute our parameter vector that minimizes the cost function. Numpy has a linear algebra module that can compute the inverse and dot product of two matricies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729f2cdbd7cd4a6888c70691b7dea0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='alpha', max=20), IntSlider(value=5, description='beta',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ipywidgets import *\n",
    "\n",
    "X = 2 * np.random.rand(100,1)\n",
    "\n",
    "def get_rand_lin_dist(alpha, beta):\n",
    "    return alpha + beta * X + np.random.randn(100, 1)\n",
    "\n",
    "def get_theta_best(X, Y):\n",
    "    X_tot = np.c_[np.ones((100,1)), X]     # Add x0 = 1 to every instance\n",
    "    return np.linalg.inv(X_tot.T.dot(X_tot)).dot(X_tot.T).dot(Y)\n",
    "\n",
    "def plt_rand_lin_dist(alpha, beta):\n",
    "    Y = get_rand_lin_dist(alpha, beta)\n",
    "    \n",
    "    X_lims = np.linspace(0, 2, num=100)\n",
    "    X_lims_tot = np.c_[np.ones((100,1)), X_lims]\n",
    "    \n",
    "    theta_closed_form_pred = get_theta_best(X, Y)\n",
    "    \n",
    "    y_pred = X_lims_tot.dot(theta_closed_form_pred)\n",
    "    \n",
    "    plt.scatter(X, Y)\n",
    "    plt.plot(X_lims, y_pred, 'r-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Random Linear Distribution')\n",
    "    plt.xlim(0, 2)\n",
    "    plt.ylim(0,20)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Closed Form Best Theta Predictions: \", theta_closed_form_pred)\n",
    "    \n",
    "interactive_plt_rand_lin_dist = interactive(plt_rand_lin_dist, \n",
    "                                            alpha=(0,20), beta=(0,10))\n",
    "interactive_plt_rand_lin_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, the closed form solution to $\\hat{\\boldsymbol{\\theta}}$ has been thrown off by the random noise.\n",
    "\n",
    "#### 4.1.3.2 Linear Regression via Scikit-Learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
